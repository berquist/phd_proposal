%% Compile me as follows:
%%% latexmk -pdf -xelatex -shell-escape presentation.tex

\documentclass[xetex,compress]{beamer}
%% \usetheme{Pittsburgh}
%% \usecolortheme{seagull}

%% https://www.quora.com/Presentations-What-are-the-best-beamer-themes
\setbeamertemplate{frametitle}
  {\begin{centering}\smallskip
   \insertframetitle\par
   \smallskip\end{centering}}
\setbeamertemplate{itemize item}{\(\bullet\)}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[text line]{%
    \hfill\strut{%
        \scriptsize\sf\color{black!60}%
        \quad\insertframenumber
    }%
    \hfill
}
%% \setbeamertemplate{caption}{\raggedright\insertcaption\par}

% Define some colors:
\definecolor{PittBlue}{RGB}{0,43,94}
\definecolor{PittGold}{RGB}{197,168,118}
\definecolor{DarkFern}{HTML}{407428}
\definecolor{DarkCharcoal}{HTML}{4D4944}
\colorlet{Fern}{DarkFern!85!white}
\colorlet{Charcoal}{DarkCharcoal!85!white}
\colorlet{LightCharcoal}{Charcoal!50!white}
\colorlet{AlertColor}{orange!80!black}
\colorlet{DarkRed}{red!70!black}
\colorlet{DarkBlue}{blue!70!black}
\colorlet{DarkGreen}{green!70!black}
% Use the colors:
\setbeamercolor{title}{fg=PittBlue}
\setbeamercolor{frametitle}{fg=PittBlue}
\setbeamercolor{normal text}{fg=DarkCharcoal}
\setbeamercolor{block title}{fg=black,bg=Fern!25!white}
\setbeamercolor{block body}{fg=black,bg=Fern!25!white}
\setbeamercolor{alerted text}{fg=AlertColor}
\setbeamercolor{itemize item}{fg=Charcoal}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{alltt}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{paralist}
\usepackage{setspace}
\usepackage{xparse}
\usepackage{braket}
\usepackage{wrapfig}
\usepackage[export]{adjustbox}
\usepackage{booktabs}

\makeatletter
\g@addto@macro\@floatboxreset\centering
\makeatother

\DeclareDocumentCommand{\vect}{m}{
        \ensuremath{\boldsymbol{\mathbf{#1}}}
}

%%% Chemistry
\usepackage{chemformula}

\makeatletter
\newcommand\listofframes{\@starttoc{lbf}}
\makeatother

\addtobeamertemplate{frametitle}{}{%
  \addcontentsline{lbf}{section}{\protect\makebox[2em][l]{%
    \protect\usebeamercolor[fg]{structure}\insertframenumber\hfill}%
  \insertframetitle\par}%
}

\setbeamertemplate{navigation symbols}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Deciphering the Contents of Chemically-Trained Neural Networks into Physical Intuition}
\author[Berquist]{Eric Berquist}
%% \institute[Pitt]{University of Pittsburgh}
\institute[Pitt]{\includegraphics[width=1in]{./figures/logo.png}}
\date{June 15th, 2017}

\begin{document}

\frame{
  \titlepage
}

\begin{frame}{Overview}
  Machine learning (ML) is seeing rapid growth in areas relevant to quantum chemistry, but how does it work?
  \begin{itemize}
  \item \underline{Topic}: Are correct ML predictions in quantum chemistry \emph{right for the right reasons}?
  \item \underline{Gap}: We don't know if current approaches (ML architectures) will work more complex molecules or properties.
  \item \underline{Rationale}: If a ML model is not right for the right reasons, there cannot be an expectation that it is transferable or extendable in any way.
  \end{itemize}
  We need to know if ML models are learning chemistry! (polynomial elephant analogy?)
\end{frame}

\begin{frame}{Overview}
  \begin{itemize}
  \item The \underline{objective} is to quantify what ML models trained on quantum chemical data are learning.
  \item The \underline{central hypothesis} is that models are learning about molecular structure identically to how we apply chemical intuition.
  \end{itemize}
  This hypothesis will be tested by
  \begin{itemize}
  \item training neural networks (NNs) to replicate literature results,
  \item ``seeing'' what the currently-available models have learned using \textbf{relevance propagation},
  \item attempt to predict more complex molecular properties than those found in the literature, and
  \item quantify if learning changes for more complex properties.
  \end{itemize}
\end{frame}

\begin{frame}{Disclaimer}
  The goal of this work is \emph{not} to produce more accurate or more transferable models. The goal is to understand \emph{how} and \emph{why} models make (in)accurate predictions in terms of what they have learned.
\end{frame}

\begin{frame}{Transferability (TODO where to put this?)}
  Literature usage:
  \begin{itemize}
  \item No need for reparametrization from system to system
  \item More to do with the \underline{input representation} than the \underline{molecules} it can be applied to
  \item Limited to organic molecules, train small (9 heavy atoms), test larger (10 heavy atoms)
  \item Charge and spin: neutral and closed-shell singlet
  \end{itemize}
  A better definition in terms of examples:
  \begin{itemize}
  \item Does the same model work for optimized and non-equilibrium (MD) structures?
  \item Does the model work for charged systems?
  \item Does the model work for systems with unpaired electrons?
  \end{itemize}
\end{frame}

\begin{frame}{What is machine learning?}
  Arthur Samuel, 1959, the subfield of computer science that gives:
  \begin{quote}
    computers the ability to learn without being explicitly programmed.
  \end{quote}
  An updated definition: (TODO where is this quote from?)
  \begin{quote}
    A computer program is said to learn from experience \textit{E} with respect to some class of tasks \textit{T} and performance measure \textit{P} if its performance at tasks in \textit{T}, as measured by \textit{P}, improves with experience \textit{E}.
  \end{quote}
\end{frame}

\begin{frame}{Machine learning will solve all our problems}
  \begin{center}
    \includegraphics[width=1.00\textwidth]{./figures/headline.png}
  \end{center}
\end{frame}

\begin{frame}{Machine learning will solve all our problems}
  \begin{center}
    \includegraphics[width=1.10\textwidth]{./figures/twitter.jpeg}
  \end{center}
\end{frame}

\begin{frame}{}
  \begin{center}
    \includegraphics[width=0.75\textwidth]{./figures/machine_learning.png}
  \end{center}
\end{frame}

\begin{frame}{Machine learning has a perception problem}
  \begin{itemize}
  \item Machine learning is a ``fad'' and produces all these great results, but we joke semi-seriously that we don't know what's going on under the hood even though it'll solve all our problems (something about robot overlords).
  \item introduce black boxes here
  \end{itemize}
\end{frame}

\begin{frame}{Need}
  \begin{itemize}
  \item Interpreting the models themselves
  \item We want the models to learn chemistry!
  \end{itemize}
\end{frame}

\begin{frame}{Rationale}
  \begin{itemize}
  \item Building ML models that can do real, useful chemistry in a general manner is impossible without proving meaningfulness (not correctness) of simpler models
  \item Clearly the dozen or so papers from 2016-2017 show that accurate predictions can be made even under the assumption of black-box models
  \item Additionally, if we can interpret the model directly, then perhaps eventually we can interpret chemistry using the model itself and not just predictions
  \end{itemize}
\end{frame}

\begin{frame}{Objective}
  \begin{itemize}
  \item Peek inside the black box and see if models are ``learning chemistry''
  \item If they aren't, consider other NN architectures (mention ANAKIN-ME, DTNN)
  \item Is it alright to accept the use of NNs that are not truly transferable (B3LYP, M06)? Maybe this works for prediction results, but we will repeat the history of DFT! (PMWG obituary)
  \end{itemize}
\end{frame}

\begin{frame}{Specific Aims}
  \begin{itemize}
  %% \item[1.] Reproduction of Existing Literature Neural Networks
  %% \item[2.] Characterization of Existing Literature Neural Networks
  %% \item[3.] Training Neural Networks for Complex Molecular Properties
  %% \item[4.] Characterization of Novel Neural Networks
  \item[1.] Reproduce existing machine learning models for molecular properties from the literature.
  \item[2.] Characterize the parameters learned by existing machine learning models from the literature.
  \item[3.] Train supervised neural networks on complex molecular properties.
  \item[4.] Characterize the parameters learned for complex molecular properties using unsupervised neural networks.
  \end{itemize}
\end{frame}

\begin{frame}{Background}
  \begin{itemize}
  \item Introduction to machine learning
  \item Simplest form: univariate linear regression
  \item Neural networks
  \item LR using a NN
  \item Pictorial representation of NN structures
  \item Training a NN
  \item Relevance propagation: examples
  \item Relevance propagation: analogies go here
  \end{itemize}
\end{frame}

\begin{frame}{Aim \#1: Reproduction of Existing Literature Neural Networks}
  \begin{itemize}
  \item Discuss GC architecture, why choose GC architecture over DTNN, ANAKIN-ME, ...?
  \item Want comparison against literature results (more on this later), these so far are molecular energies only
  \item Where is the code?
  \item Why \emph{not} look at molecular energies? Want ML to do spectroscopy too, calculations for which are much more expensive than energies/trajectories.
  \end{itemize}
\end{frame}

\begin{frame}{Aim \#2: Characterization of Existing Literature Neural Networks}
  \begin{itemize}
  \item Math w/ example pictures
  \item Expected outcomes
  \end{itemize}
\end{frame}

\begin{frame}{Aim \#3: Training Neural Networks for Complex Molecular Properties}
  \begin{itemize}
  \item What are the properties, logic behind the choice
  \item Expected outcomes
  \end{itemize}
\end{frame}

\begin{frame}{Aim \#4: Characterization of Novel Neural Networks}
  \begin{itemize}
  \item Unsupervised learning (PCA analogy)
  \item Denoising autoencoder
  \item Expected outcomes
  \end{itemize}
\end{frame}

\begin{frame}{Approximate Timeline}
\begin{table}[]
\centering
\begin{tabular}{@{}rll@{}}
\toprule
Specific Aim & Task                                    & \# of Months \\ \midrule
1            & code development: forming pipeline      & 2            \\
1            & model training                          & 1-2          \\
2            & code development: adapt LRP to pipeline & 2            \\
2            & analysis development                    & 1            \\
3            & hyperpolarizability calculations        & 1-2          \\
3            & model training                          & 2-3          \\
4            & code development: DAE                   & 2            \\
4            & model training                          & 2            \\
4            & analysis                                & 1            \\ \midrule
Total        &                                         & 14-17        \\ \bottomrule
\end{tabular}
\end{table}\end{frame}

\begin{frame}{A future challenge: building databases}
  \begin{itemize}
  \item GDB9/QM9 is the most commonly-used training set, the equivalent of the MNIST set of \~10,000 labeled handwritten digits.
  \item It is now suffering from the same problem as MNIST: it is too simple and not representative of real-world training cases (molecules).
  \item Analogy: Pople basis sets (6-31G and derivatives) are still extremely common, not even because we don't know better, but because we ``need to compare to past work''.
  \item If a \emph{general and transferable} ML model fails on GDB9, that is a warning sign, but the above cannot be a reason against extending deeper into chemical space for ML model training.
  \end{itemize}
\end{frame}

\begin{frame}{An (imperfect) analogy between neural networks and quantum chemistry}
  \begin{itemize}
  \item The fundamental components of the network (kind of neuron activation functions, convolution or direct connection) are like the \emph{Hamiltonian}.
  \item The number of components in each network layer and the number of layers are like the size of the \emph{basis set}.
  \end{itemize}
\end{frame}

\begin{frame}{A better analogy}
  between relevance propagation and interaction energy decomposition approaches (SAPT, EDA):
  \begin{itemize}
  \item I am an item
  \end{itemize}
\end{frame}

%% definition of one-hot

%% Changing the number of layers and number of nodes per layer is like altering the variational upper bound of the network
%% Weights are like MO coefficients

%% How does LRP prove that a NN is transferable?

%% Find the picture of the horse where importance was placed on the text in the corner

%% Review of the relevant literature I missed: ANI-1, DTNN

\begin{frame}{Definitions of trained molecular properties}
  \begin{itemize}
  \item Zero-point (vibrational) energy: \(E_{\text{ZPVE}} = \frac{1}{2} h \sum_{i}^{\text{normal modes}} \nu_{i}\)
  \item Isotropic polarizability (static, \(\omega = 0\)): \(\alpha_{\text{iso}} = \bar{\alpha} \equiv \frac{1}{3} (\alpha_{xx} + \alpha_{yy} + \alpha_{zz})\)
  \item Parallel 1st hyperpolarizability (static, \(\omega_{a} = \omega_{b} = 0\)): \(\beta_{\parallel} \equiv\)
  \end{itemize}
\end{frame}

%% \begin{frame}{}
%% \fxnote{Also, what to do in the event that the lit model isn't reproducible? It may indicate that not enough information is given in the literature: publications are notorious for not providing enough information about code/model parameters for reproduction. Even in the event of supposed failure to fall within the error metric, the architecture is still a valuable building block. Is it safe to assume that the PI (me) will not make any mistakes in the implementation? Another possible point of failure may be due to the non-convex optimization problem posed by NNs. What is the likelihood of training a network into a local minimum?}
%% \end{frame}

\begin{frame}{}
  \begin{itemize}
    \item This is \emph{not} a direct inspection of what the NN has learned!
    \item Looking directly at NN weights is like looking at MO coefficients. Once the number of them grows, the ``importance'' of a single one diminishes greatly, and the number of nodes/weights grows even quicker than the number of MO coefficients for a reasonable quantum chemical calculation. The ability for direct inspection becomes impossible.
    \item Toy models are unlikely to be useful for any kind of understanding the effect of chemical data on NNs because of the complexity of \emph{any} molecule compared to NNs. In a way, a toy or model molecule w/ ab initio calculation can give more insight than a model NN. We are asking NN parameters to be both more efficient and more general than MO coefficients at describing the many-particle wavefunction!
  \end{itemize}
\end{frame}

%% most likely diminishes greatly. If one is particularly important, how to identify it? Do the weights form a distribution that can be inspected?

\begin{frame}{Backup Slides}
\end{frame}

\begin{frame}{Hyperpolarizability equations from paper}
  When the dipole moment coincides with the \(j\)-axis, we have
  \begin{equation}
    \beta_{\parallel} = \frac{3}{5}\beta_j = \frac{1}{5} \sum_{i=x,y,z} (\beta_{iij} + \beta_{iji} + \beta_{jii}),
  \end{equation}
  or in the general case,
  \begin{equation}
    \beta_{\parallel} = \frac{3}{5|\mu|} \sum_{i,j=x,y,z} \beta_{iij} \mu_{j},
  \end{equation}
  where
  \begin{equation}
    \beta_{ijk} = \left<\left<\mu_{i};\mu_{j},\mu_{k}\right>\right>.
  \end{equation}
\end{frame}

\end{document}

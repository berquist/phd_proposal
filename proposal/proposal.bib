% Encoding: UTF-8

@InProceedings{VincentPLarochelleH2008,
  author   = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  title    = {Extracting and Composing Robust Features with Denoising Autoencoders},
  date     = {2008},
  pages    = {1096--1103},
  abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization
for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In
this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for
RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This
approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.},
  crossref = {ICML08},
}

@InCollection{NIPS2012_4830,
  author    = {Montavon, Gr\'{e}goire and Katja Hansen and Siamac Fazli and Matthias Rupp and Franziska Biegler and Andreas Ziehe and Alexandre Tkatchenko and Anatole V. Lilienfeld and M\"{u}ller, Klaus-Robert},
  title     = {Learning Invariant Representations of Molecules for Atomization Energy Prediction},
  booktitle = {Advances in Neural Information Processing Systems 25},
  date      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {440--448},
  url       = {http://papers.nips.cc/paper/4830-learning-invariant-representations-of-molecules-for-atomization-energy-prediction.pdf},
}
﻿
@Article{2017arXiv170205532F,
  author       = {{Faber}, F.~A. and {Hutchison}, L. and {Huang}, B. and {Gilmer}, J. and {Schoenholz}, S.~S. and {Dahl}, G.~E. and {Vinyals}, O. and {Kearnes}, S. and {Riley}, P.~F. and {Anatole von Lilienfeld}, O.},
  title        = {{Fast machine learning models of electronic and energetic properties consistently reach approximation errors better than DFT accuracy}},
  journaltitle = {ArXiv e-prints},
  date         = {2017-02},
  eprint       = {1702.05532},
  eprintclass  = {physics.chem-ph},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2017arXiv170205532F},
  keywords     = {Physics - Chemical Physics},
}

@Article{2017arXiv170401212G,
  author       = {{Gilmer}, J. and {Schoenholz}, S.~S. and {Riley}, P.~F. and {Vinyals}, O. and {Dahl}, G.~E.},
  title        = {{Neural Message Passing for Quantum Chemistry}},
  journaltitle = {ArXiv e-prints},
  date         = {2017-04},
  eprint       = {1704.01212},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2017arXiv170401212G},
  keywords     = {Computer Science - Learning, I.2.6},
}

@online{wiki:blackbox,
url = {https://en.wikipedia.org/wiki/Black_box},
urldate = {2017-03-08},
}

@online{wiki:uat,
url = {https://en.wikipedia.org/wiki/Universal_approximation_theorem},
urldate = {2017-06-07},
}

@online{blog:jblevins,
author = {Jason Blevins},
url = {http://jblevins.org/log/rep},
urldate = {2017-03-08},
}

@Article{QCHEM4,
  author       = {Y. Shao and Z. Gan and E. Epifanovsky and A. T. B. Gilbert and M. Wormit and J. Kussmann and A. W. Lange and A. Behn and J. Deng and X. Feng and D. Ghosh and M. Goldey and P. R. Horn and L. D. Jacobson and I. Kaliman and R. Z. Khaliullin and T. K\'us and A. Landau and J. Liu and E. I. Proynov and Y. M. Rhee and R. M. Richard and M. A. Rohrdanz and R. P. Steele and E. J. Sundstrom and H. L. {Woodcock III} and P. M. Zimmerman and D. Zuev and B. Albrecht and E. Alguire and B. Austin and G. J. O. Beran and Y. A. Bernard and E. Berquist and K. Brandhorst and K. B. Bravaya and S. T. Brown and D. Casanova and C.-M. Chang and Y. Chen and S. H. Chien and K. D. Closser and D. L. Crittenden and M. Diedenhofen and R. A. {DiStasio Jr.} and H. Dop and A. D. Dutoi and R. G. Edgar and S. Fatehi and L. {Fusti-Molnar} and A. Ghysels and A. {Golubeva-Zadorozhnaya} and J. Gomes and M. W. D. {Hanson-Heine} and P. H. P. Harbach and A. W. Hauser and E. G. Hohenstein and Z. C. Holden and T.-C. Jagau and H. Ji and B. Kaduk and K. Khistyaev and J. Kim and J. Kim and R. A. King and P. Klunzinger and D. Kosenkov and T. Kowalczyk and C. M. Krauter and K. U. Lao and A. Laurent and K. V. Lawler and S. V. Levchenko and C. Y. Lin and F. Liu and E. Livshits and R. C. Lochan and A. Luenser and P. Manohar and S. F. Manzer and S.-P. Mao and N. Mardirossian and A. V. Marenich and S. A. Maurer and N. J. Mayhall and C. M. Oana and R. {Olivares-Amaya} and D. P. O'Neill and J. A. Parkhill and T. M. Perrine and R. Peverati and P. A. Pieniazek and A. Prociuk and D. R. Rehn and E. Rosta and N. J. Russ and N. Sergueev and S. M. Sharada and S. Sharmaa and D. W. Small and A. Sodt and T. Stein and D. St\"uck and Y.-C. Su and A. J. W. Thom and T. Tsuchimochi and L. Vogt and O. Vydrov and T. Wang and M. A. Watson and J. Wenzel and A. White and C. F. Williams and V. Vanovschi and S. Yeganeh and S. R. Yost and Z.-Q. You and I. Y. Zhang and X. Zhang and Y. Zhou and B. R. Brooks and G. K. L. Chan and D. M. Chipman and C. J. Cramer and W. A. {Goddard III} and M. S. Gordon and W. J. Hehre and A. Klamt and H. F. {Schaefer III} and M. W. Schmidt and C. D. Sherrill and D. G. Truhlar and A. Warshel and X. Xua and A. {Aspuru-Guzik} and R. Baer and A. T. Bell and N. A. Besley and J.-D. Chai and A. Dreuw and B. D. Dunietz and T. R. Furlani and S. R. Gwaltney and C.-P. Hsu and Y. Jung and J. Kong and D. S. Lambrecht and W. Liang and C. Ochsenfeld and V. A. Rassolov and L. V. Slipchenko and J. E. Subotnik and T. {Van Voorhis} and J. M. Herbert and A. I. Krylov and P. M. W. Gill and M. {Head-Gordon}},
  title        = {Advances in molecular quantum chemistry contained in the Q-Chem 4 program package},
  journaltitle = {Mol.\ Phys.},
  date         = {2015},
  volume       = {113},
  pages        = {184--215},
}

@Article{WCMS:WCMS93,
  author       = {Turney, Justin M. and Simmonett, Andrew C. and Parrish, Robert M. and Hohenstein, Edward G. and Evangelista, Francesco A. and Fermann, Justin T. and Mintz, Benjamin J. and Burns, Lori A. and Wilke, Jeremiah J. and Abrams, Micah L. and Russ, Nicholas J. and Leininger, Matthew L. and Janssen, Curtis L. and Seidl, Edward T. and Allen, Wesley D. and Schaefer, Henry F. and King, Rollin A. and Valeev, Edward F. and Sherrill, C. David and Crawford, T. Daniel},
  title        = {Psi4: an open-source ab initio electronic structure program},
  journaltitle = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
  date         = {2012},
  volume       = {2},
  number       = {4},
  pages        = {556--565},
  issn         = {1759-0884},
  doi          = {10.1002/wcms.93},
  publisher    = {John Wiley \& Sons, Inc.},
}

@Article{Ramakrishnan:2014ij,
  author        = {Ramakrishnan, Raghunathan and Dral, Pavlo O and Rupp, Matthias and von Lilienfeld, O Anatole},
  title         = {{Quantum chemistry structures and properties of 134 kilo molecules}},
  journaltitle  = {Scientific Data},
  date          = {2014-08},
  volume        = {1},
  doi           = {10.1038/sdata.2014.22},
  url           = {http://www.nature.com/articles/sdata201422},
  date-added    = {2016-11-26T23:21:00GMT},
  date-modified = {2017-03-08T20:23:58GMT},
  file          = {{10.1038sdata.2014.22 Sci. Data 2014 Ramakrishnan.pdf:/Users/flareon/Papers2/Articles/2014/Ramakrishnan/10.1038sdata.2014.22 Sci. Data 2014 Ramakrishnan.pdf:application/pdf}},
  local-url     = {file://localhost/Users/flareon/Papers2/Articles/2014/Ramakrishnan/10.1038sdata.2014.22%20Sci.%20Data%202014%20Ramakrishnan.pdf},
  rating        = {0},
  read          = {Yes},
  uri           = {\url{papers2://publication/doi/10.1038/sdata.2014.22}},
}

@Article{daltonpaper,
  author       = {K\k{e}stutis Aidas and Celestino Angeli and Keld L. Bak and Vebj\o{}rn Bakken and Radovan Bast and Linus Boman and Ove Christiansen and Renzo Cimiraglia and Sonia Coriani and P\aa{}l Dahle and Erik K. Dalskov and Ulf Ekstr\"{o}m and Thomas Enevoldsen and Janus J. Eriksen and Patrick Ettenhuber and Berta Fern\'{a}ndez and Lara Ferrighi and Heike Fliegl and Luca Frediani and Kasper Hald and Asger Halkier and Christof H\"{a}ttig and Hanne Heiberg and Trygve Helgaker and Alf Christian Hennum and Hinne Hettema and Eirik Hjerten\ae{}s and Stinne H\o{}st and Ida-Marie H\o{}yvik and Maria Francesca Iozzi and Branislav Jans\'{i}k and Hans J\o{}rgen {\relax Aa}.~Jensen and Dan Jonsson and Poul J\o{}rgensen and Joanna Kauczor and Sheela Kirpekar and Thomas Kj\ae{}rgaard and Wim Klopper and Stefan Knecht and Rika Kobayashi and Henrik Koch and Jacob Kongsted and Andreas Krapp and Kasper Kristensen and Andrea Ligabue and Ola B Lutn\ae{}s and Juan I. Melo and Kurt V. Mikkelsen and Rolf H. Myhre and Christian Neiss and Christian B. Nielsen and Patrick Norman and Jeppe Olsen and J\'{o}gvan Magnus H. Olsen and Anders Osted and Martin J. Packer and Filip Pawlowski and Thomas B. Pedersen and Patricio F. Provasi and Simen Reine and Zilvinas Rinkevicius and Torgeir A. Ruden and Kenneth Ruud and Vladimir V. Rybkin and Pawel Sa\l{}ek and Claire C. M. Samson and Alfredo S\'{a}nchez de Mer\'{a}s and Trond Saue and Stephan P. A. Sauer and Bernd Schimmelpfennig and Kristian Sneskov and Arnfinn H. Steindal and Kristian O. Sylvester-Hvid and Peter R. Taylor and Andrew M. Teale and Erik I. Tellgren and David P. Tew and Andreas J. Thorvaldsen and Lea Th\o{}gersen and Olav Vahtras and Mark A. Watson and David J. D. Wilson and Marcin Ziolkowski and Hans \AA{}gren},
  title        = {{The Dalton quantum chemistry program system}},
  journaltitle = {WIREs Comput.~Mol.~Sci.},
  date         = {2014},
  volume       = {4},
  number       = {3},
  pages        = {269--284},
  doi          = {10.1002/wcms.1172},
}

@Misc{g16,
  author = {M. J. Frisch and G. W. Trucks and H. B. Schlegel and G. E. Scuseria and M. A. Robb and J. R. Cheeseman and G. Scalmani and V. Barone and G. A. Petersson and H. Nakatsuji and X. Li and M. Caricato and A. V. Marenich and J. Bloino and B. G. Janesko and R. Gomperts and B. Mennucci and H. P. Hratchian and J. V. Ortiz and A. F. Izmaylov and J. L. Sonnenberg and D. Williams-Young and F. Ding and F. Lipparini and F. Egidi and J. Goings and B. Peng and A. Petrone and T. Henderson and D. Ranasinghe and V. G. Zakrzewski and J. Gao and N. Rega and G. Zheng and W. Liang and M. Hada and M. Ehara and K. Toyota and R. Fukuda and J. Hasegawa and M. Ishida and T. Nakajima and Y. Honda and O. Kitao and H. Nakai and T. Vreven and K. Throssell and Montgomery, {Jr.}, J. A. and J. E. Peralta and F. Ogliaro and M. J. Bearpark and J. J. Heyd and E. N. Brothers and K. N. Kudin and V. N. Staroverov and T. A. Keith and R. Kobayashi and J. Normand and K. Raghavachari and A. P. Rendell and J. C. Burant and S. S. Iyengar and J. Tomasi and M. Cossi and J. M. Millam and M. Klene and C. Adamo and R. Cammi and J. W. Ochterski and R. L. Martin and K. Morokuma and O. Farkas and J. B. Foresman and D. J. Fox},
  title  = {Gaussian˜16 {R}evision {A}.03},
  date   = {2016},
  note   = {Gaussian Inc. Wallingford CT},
}

@Article{C6NJ02492K,
  author       = {Yadav, Maneesh K.},
  title        = {On the synthesis of machine learning and automated reasoning for an artificial synthetic organic chemist},
  journaltitle = {New J. Chem.},
  date         = {2017},
  volume       = {41},
  issue        = {4},
  pages        = {1411-1416},
  doi          = {10.1039/C6NJ02492K},
  abstract     = {This perspective outlines current capabilities and limitations of state-of-the-art artificial intelligence methods as applied to automating the planning of synthetic routes in organic chemistry. Synthetic organic chemistry is viewed from the perspective of two prominent approaches: deep neural networks and SAT-solver based automated reasoning. After introducing these concepts to non-computer scientists{,} the expected performance of these approaches is estimated by surveying comparable problems in artificial intelligence. A truly artificial synthetic organic chemist that automatically constructs viable synthetic routes is clearly a challenging artificial intelligence problem and not directly amenable to existing approaches but chemistry could encourage new combinations of machine learning methods with automated reasoning to realize this goal. The importance of objective and open competitions with standardized problems and evaluations is also detailed as critical to realizing tangible computer programs that automate the planning of plausible synthetic routes.},
  publisher    = {The Royal Society of Chemistry},
}

@Article{doi:10.1021/ci500747n,
  author       = {Ma, Junshui and Sheridan, Robert P. and Liaw, Andy and Dahl, George E. and Svetnik, Vladimir},
  title        = {Deep Neural Nets as a Method for Quantitative Structure–Activity Relationships},
  journaltitle = {Journal of Chemical Information and Modeling},
  date         = {2015},
  volume       = {55},
  number       = {2},
  pages        = {263-274},
  note         = {PMID: 25635324},
  doi          = {10.1021/ci500747n},
  eprint       = {http://dx.doi.org/10.1021/ci500747n},
}

@Article{THOMSEN1989212,
  author       = {J.U Thomsen and B Meyer},
  title        = {Pattern recognition of the 1H NMR spectra of sugar alditols using a neural network},
  journaltitle = {Journal of Magnetic Resonance (1969)},
  date         = {1989},
  volume       = {84},
  number       = {1},
  pages        = {212 - 217},
  issn         = {0022-2364},
  doi          = {10.1016/0022-2364(89)90021-8},
  url          = {http://www.sciencedirect.com/science/article/pii/0022236489900218},
}

@Article{doi:10.1080/17460441.2016.1201262,
  author       = {Igor I. Baskin and David Winkler and Igor V. Tetko},
  title        = {A renaissance of neural networks in drug discovery},
  journaltitle = {Expert Opinion on Drug Discovery},
  date         = {2016},
  volume       = {11},
  number       = {8},
  pages        = {785-795},
  note         = {PMID: 27295548},
  doi          = {10.1080/17460441.2016.1201262},
  eprint       = {http://dx.doi.org/10.1080/17460441.2016.1201262},
}

@Article{Ramakrishnan_2014_140022,
  author       = {Ramakrishnan, Raghunathan and Dral, Pavlo O. and Rupp, Matthias and von Lilienfeld, O. Anatole},
  title        = {Quantum chemistry structures and properties of 134 kilo molecules},
  journaltitle = {Scientific Data},
  date         = {2014-08},
  volume       = {1},
  pages        = {140022},
  doi          = {10.1038/sdata.2014.22},
  publisher    = {The Author(s)},
  timestamp    = {2017.04.21},
}

@Article{doi:10.1021/ci300415d,
  author       = {Ruddigkeit, Lars and van Deursen, Ruud and Blum, Lorenz C. and Reymond, Jean-Louis},
  title        = {Enumeration of 166 Billion Organic Small Molecules in the Chemical Universe Database GDB-17},
  journaltitle = {Journal of Chemical Information and Modeling},
  date         = {2012},
  volume       = {52},
  number       = {11},
  pages        = {2864-2875},
  note         = {PMID: 23088335},
  doi          = {10.1021/ci300415d},
  eprint       = {http://dx.doi.org/10.1021/ci300415d},
}

@Article{Kearnes2016,
  author       = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
  title        = {Molecular graph convolutions: moving beyond fingerprints},
  journaltitle = {Journal of Computer-Aided Molecular Design},
  date         = {2016},
  volume       = {30},
  number       = {8},
  pages        = {595--608},
  issn         = {1573-4951},
  doi          = {10.1007/s10822-016-9938-8},
  abstract     = {Molecular ``fingerprints'' encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph---atoms, bonds, distances, etc.---which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.},
}

@Article{2015arXiv151105493L,
  author       = {{Li}, Y. and {Tarlow}, D. and {Brockschmidt}, M. and {Zemel}, R.},
  title        = {{Gated Graph Sequence Neural Networks}},
  journaltitle = {ArXiv e-prints},
  date         = {2015-11},
  eprint       = {1511.05493},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2015arXiv151105493L},
  keywords     = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@Article{10.1371/journal.pone.0130140,
  author       = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
  title        = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  journaltitle = {PLOS ONE},
  date         = {2015-07},
  volume       = {10},
  number       = {7},
  pages        = {1-46},
  doi          = {10.1371/journal.pone.0130140},
  abstract     = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  publisher    = {Public Library of Science},
}

@Article{doi:10.1063/1.2436891,
  author       = {Karl K. Irikura},
  title        = {Experimental Vibrational Zero-Point Energies: Diatomic Molecules},
  journaltitle = {Journal of Physical and Chemical Reference Data},
  date         = {2007},
  volume       = {36},
  number       = {2},
  pages        = {389-397},
  doi          = {10.1063/1.2436891},
  eprint       = {http://dx.doi.org/10.1063/1.2436891},
}

@Article{POC:POC407,
  author       = {Soscún, Humberto and Alvarado, Ysaías and Hernández, Javier and Hernández, Paola and Atencio, Reinaldo and Hinchliffe, Alan},
  title        = {Experimental and theoretical determination of the dipole polarizability of dibenzothiophene},
  journaltitle = {Journal of Physical Organic Chemistry},
  date         = {2001},
  volume       = {14},
  number       = {10},
  pages        = {709--715},
  issn         = {1099-1395},
  doi          = {10.1002/poc.407},
  keywords     = {dibenzothiophene, dipole polarizability, refractometry, density functional theory},
  publisher    = {John Wiley \& Sons, Ltd.},
}

@WWW{rdkit,
  title     = {RDKit: Open-source cheminformatics},
  url       = {http://www.rdkit.org},
  urldate   = {2017-05-07},
  timestamp = {2017-05-07},
}

@Misc{tensorflow2015-whitepaper,
  author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  date   = {2015},
  note   = {Software available from tensorflow.org},
  url    = {http://tensorflow.org/},
}

@Article{kipf2016semi,
  author       = {Kipf, Thomas N and Welling, Max},
  title        = {Semi-Supervised Classification with Graph Convolutional Networks},
  journaltitle = {arXiv preprint arXiv:1609.02907},
  date         = {2016},
}

@Article{scikit-learn,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title        = {Scikit-learn: Machine Learning in {P}ython},
  journaltitle = {Journal of Machine Learning Research},
  date         = {2011},
  volume       = {12},
  pages        = {2825--2830},
}

@InProceedings{lecun-01a,
  author    = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  title     = {Gradient-Based Learning Applied to Document Recognition},
  booktitle = {Intelligent Signal Processing},
  date      = {2001},
  publisher = {IEEE Press},
  pages     = {306-351},
  editors   = {Haykin, S. and Kosko, B.},
  original  = {orig/lecun-01a.ps.gz},
}

@Article{JMLR:v17:15-618,
  author       = {Sebastian Lapuschkin and Alexander Binder and Gr{{\'e}}goire Montavon and Klaus-Robert M{{{\"u}}}ller and Wojciech Samek},
  title        = {The LRP Toolbox for Artificial Neural Networks},
  journaltitle = {Journal of Machine Learning Research},
  date         = {2016},
  volume       = {17},
  number       = {114},
  pages        = {1-5},
  url          = {http://jmlr.org/papers/v17/15-618.html},
}

@Online{blog:dnn2,
  url     = {https://dmm613.wordpress.com/2014/12/16/demystifying-artificial-neural-networks-part-2/},
  urldate = {2017-05-15},
}

@Online{nvidia-ml,
  date      = {2017-05-16},
  url       = {http://www.nvidia.com/object/machine-learning.html},
  timestamp = {2017.05.16},
}

@Article{2017arXiv170505907G,
  author       = {{Gastegger}, M. and {Behler}, J. and {Marquetand}, P.},
  title        = {{Machine Learning Molecular Dynamics for the Simulation of Infrared Spectra}},
  journaltitle = {ArXiv e-prints},
  date         = {2017-05},
  eprint       = {1705.05907},
  eprintclass  = {physics.chem-ph},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2017arXiv170505907G},
  keywords     = {Physics - Chemical Physics},
}

@Article{Finnegan105957,
  author       = {Finnegan, Alex I and Song, Jun S},
  title        = {Maximum Entropy Methods for Extracting the Learned Features of Deep Neural Networks},
  journaltitle = {bioRxiv},
  date         = {2017},
  doi          = {10.1101/105957},
  eprint       = {http://biorxiv.org/content/early/2017/02/03/105957.full.pdf},
  url          = {http://biorxiv.org/content/early/2017/02/03/105957},
  abstract     = {New architectures of multilayer artificial neural networks and new methods for training them are rapidly revolutionizing the application of machine learning in diverse fields, including business, social science, physical sciences, and biology. Interpreting deep neural networks, however, currently remains elusive, and a critical challenge lies in understanding which meaningful features a network is actually learning. We present a general method for interpreting deep neural networks and extracting network-learned features from input data. We describe our algorithm in the context of biological sequence analysis. Our approach, based on ideas from statistical physics, samples from the maximum entropy distribution over possible sequences, anchored at an input sequence and subject to constraints implied by the empirical function learned by a network. Using our framework, we demonstrate that local transcription factor binding motifs can be identified from a network trained on ChIP-seq data and that nucleosome positioning signals are indeed learned by a network trained on chemical cleavage nucleosome maps. Imposing a further constraint on the maximum entropy distribution, similar to the grand canonical ensemble in statistical physics, also allows us to probe whether a network is learning global sequence features, such as the high GC content in nucleosome-rich regions. This work thus provides valuable mathematical tools for interpreting and extracting learned features from feed-forward neural networks.},
  publisher    = {Cold Spring Harbor Labs Journals},
}

@Article{2017arXiv170303717S,
  author       = {{Slavin Ross}, A. and {Hughes}, M.~C. and {Doshi-Velez}, F.},
  title        = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
  journaltitle = {ArXiv e-prints},
  date         = {2017-03},
  eprint       = {1703.03717},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2017arXiv170303717S},
  keywords     = {Computer Science - Learning, Statistics - Machine Learning},
}

@Article{Zhao2008,
  author       = {Zhao, Yan and Truhlar, Donald G.},
  title        = {The M06 suite of density functionals for main group thermochemistry, thermochemical kinetics, noncovalent interactions, excited states, and transition elements: two new functionals and systematic testing of four M06-class functionals and 12 other functionals},
  journaltitle = {Theoretical Chemistry Accounts},
  date         = {2008},
  volume       = {120},
  number       = {1},
  pages        = {215--241},
  issn         = {1432-2234},
  doi          = {10.1007/s00214-007-0310-x},
  abstract     = {We present two new hybrid meta exchange- correlation functionals, called M06 and M06-2X. The M06 functional is parametrized including both transition metals and nonmetals, whereas the M06-2X functional is a high-nonlocality functional with double the amount of nonlocal exchange (2X), and it is parametrized only for nonmetals.The functionals, along with the previously published M06-L local functional and the M06-HF full-Hartree--Fock functionals, constitute the M06 suite of complementary functionals. We assess these four functionals by comparing their performance to that of 12 other functionals and Hartree--Fock theory for 403 energetic data in 29 diverse databases, including ten databases for thermochemistry, four databases for kinetics, eight databases for noncovalent interactions, three databases for transition metal bonding, one database for metal atom excitation energies, and three databases for molecular excitation energies. We also illustrate the performance of these 17 methods for three databases containing 40 bond lengths and for databases containing 38 vibrational frequencies and 15 vibrational zero point energies. We recommend the M06-2X functional for applications involving main-group thermochemistry, kinetics, noncovalent interactions, and electronic excitation energies to valence and Rydberg states. We recommend the M06 functional for application in organometallic and inorganometallic chemistry and for noncovalent interactions.},
}

@Article{2016arXiv161107478L,
  author       = {{Lundberg}, S. and {Lee}, S.-I.},
  title        = {{An unexpected unity among methods for interpreting model predictions}},
  journaltitle = {ArXiv e-prints},
  date         = {2016-11},
  eprint       = {1611.07478},
  eprintclass  = {cs.AI},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2016arXiv161107478L},
  keywords     = {Computer Science - Artificial Intelligence},
}

@Article{doi:10.1063/1.4973380,
  author       = {Kun Yao and John E. Herr and John Parkhill},
  title        = {The many-body expansion combined with neural networks},
  journaltitle = {The Journal of Chemical Physics},
  date         = {2017},
  volume       = {146},
  number       = {1},
  pages        = {014106},
  doi          = {10.1063/1.4973380},
  eprint       = {http://dx.doi.org/10.1063/1.4973380},
}

@Article{Medvedev496,
  author       = {Medvedev, Michael G. and Bushmarinov, Ivan S. and Sun, Jianwei and Perdew, John P. and Lyssenko, Konstantin A.},
  title        = {Response to Comment on {\textquotedblleft}Density functional theory is straying from the path toward the exact functional{\textquotedblright}},
  journaltitle = {Science},
  date         = {2017},
  volume       = {356},
  number       = {6337},
  pages        = {496--496},
  issn         = {0036-8075},
  doi          = {10.1126/science.aam9550},
  eprint       = {http://science.sciencemag.org/content/356/6337/496.3.full.pdf},
  url          = {http://science.sciencemag.org/content/356/6337/496.3},
  abstract     = {Kepp argues in his Comment, among other concerns, that the atomic densities we have considered are not relevant to molecular bonding. However, this does not change the main conclusion of our study, that unconstrained fitting of flexible functional forms can make a density functional more interpolative but less widely predictive.},
  publisher    = {American Association for the Advancement of Science},
}

@Article{Medvedev49,
  author       = {Medvedev, Michael G. and Bushmarinov, Ivan S. and Sun, Jianwei and Perdew, John P. and Lyssenko, Konstantin A.},
  title        = {Density functional theory is straying from the path toward the exact functional},
  journaltitle = {Science},
  date         = {2017},
  volume       = {355},
  number       = {6320},
  pages        = {49--52},
  issn         = {0036-8075},
  doi          = {10.1126/science.aah5975},
  eprint       = {http://science.sciencemag.org/content/355/6320/49.full.pdf},
  url          = {http://science.sciencemag.org/content/355/6320/49},
  abstract     = {The continuing development of density functional theory (DFT) has greatly expanded the size and complexity of molecules amenable to computationally tractable simulation. The conventional metric of success for new functionals has been the accuracy of their calculated energies. Medvedev et al. examined how well these functionals calculate electron density across a series of neutral and cationic atoms (see the Perspective by Hammes-Schiffer). Although historically the accuracies of energy and density have improved in tandem, certain recent functionals have sacrificed fidelity to the true density.Science, this issue p. 49; see also p. 28The theorems at the core of density functional theory (DFT) state that the energy of a many-electron system in its ground state is fully defined by its electron density distribution. This connection is made via the exact functional for the energy, which minimizes at the exact density. For years, DFT development focused on energies, implicitly assuming that functionals producing better energies become better approximations of the exact functional. We examined the other side of the coin: the energy-minimizing electron densities for atomic species, as produced by 128 historical and modern DFT functionals. We found that these densities became closer to the exact ones, reflecting theoretical advances, until the early 2000s, when this trend was reversed by unconstrained functionals sacrificing physical rigor for the flexibility of empirical fitting.},
  publisher    = {American Association for the Advancement of Science},
}

@Article{Kepp496,
  author       = {Kepp, Kasper P.},
  title        = {Comment on {\textquotedblleft}Density functional theory is straying from the path toward the exact functional{\textquotedblright}},
  journaltitle = {Science},
  date         = {2017},
  volume       = {356},
  number       = {6337},
  pages        = {496--496},
  issn         = {0036-8075},
  doi          = {10.1126/science.aam9364},
  eprint       = {http://science.sciencemag.org/content/356/6337/496.2.full.pdf},
  url          = {http://science.sciencemag.org/content/356/6337/496.2},
  abstract     = {Medvedev et al. (Reports, 6 January 2017, p. 49) argue that recent density functionals stray from the path toward exactness. This conclusion rests on very compact 1s2 and 1s22s2 systems favored by the Hartree-Fock picture. Comparison to actual energies for the same systems indicates that the {\textquotedblleft}straying{\textquotedblright} is not chemically relevant and is at best specific to the studied dense systems.},
  publisher    = {American Association for the Advancement of Science},
}

@Online{wiki:db,
  url     = {https://en.wikipedia.org/wiki/Decision_boundary},
  urldate = {2017-05-20},
}

@Article{doi:10.1021/acscentsci.6b00219,
  author       = {Wei, Jennifer N. and Duvenaud, David and Aspuru-Guzik, Alán},
  title        = {Neural Networks for the Prediction of Organic Chemistry Reactions},
  journaltitle = {ACS Central Science},
  date         = {2016},
  volume       = {2},
  number       = {10},
  pages        = {725-732},
  note         = {PMID: 27800555},
  doi          = {10.1021/acscentsci.6b00219},
  eprint       = {http://dx.doi.org/10.1021/acscentsci.6b00219},
}

@Article{C5SC04786B,
  author       = {Hase, Florian and Valleau, Stephanie and Pyzer-Knapp, Edward and Aspuru-Guzik, Alan},
  title        = {Machine learning exciton dynamics},
  journaltitle = {Chem. Sci.},
  date         = {2016},
  volume       = {7},
  issue        = {8},
  pages        = {5139-5147},
  doi          = {10.1039/C5SC04786B},
  abstract     = {Obtaining the exciton dynamics of large photosynthetic complexes by using mixed quantum mechanics/molecular mechanics (QM/MM) is computationally demanding. We propose a machine learning technique{,} multi-layer perceptrons{,} as a tool to reduce the time required to compute excited state energies. With this approach we predict time-dependent density functional theory (TDDFT) excited state energies of bacteriochlorophylls in the Fenna-Matthews-Olson (FMO) complex. Additionally we compute spectral densities and exciton populations from the predictions. Different methods to determine multi-layer perceptron training sets are introduced{,} leading to several initial data selections. In addition{,} we compute spectral densities and exciton populations. Once multi-layer perceptrons are trained{,} predicting excited state energies was found to be significantly faster than the corresponding QM/MM calculations. We showed that multi-layer perceptrons can successfully reproduce the energies of QM/MM calculations to a high degree of accuracy with prediction errors contained within 0.01 eV (0.5\%). Spectral densities and exciton dynamics are also in agreement with the TDDFT results. The acceleration and accurate prediction of dynamics strongly encourage the combination of machine learning techniques with ab initio methods.},
  publisher    = {The Royal Society of Chemistry},
}

@Article{2015arXiv150204563R,
  author       = {{Ramakrishnan}, R. and {Anatole von Lilienfeld}, O.},
  title        = {{Many Molecular Properties from One Kernel in Chemical Space}},
  journaltitle = {ArXiv e-prints},
  date         = {2015-02},
  eprint       = {1502.04563},
  eprintclass  = {physics.chem-ph},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2015arXiv150204563R},
  keywords     = {Physics - Chemical Physics},
}

@Article{2015arXiv151007512R,
  author       = {{Ramakrishnan}, R. and {Anatole von Lilienfeld}, O.},
  title        = {{Machine Learning, Quantum Mechanics, and Chemical Compound Space}},
  journaltitle = {ArXiv e-prints},
  date         = {2015-10},
  eprint       = {1510.07512},
  eprintclass  = {physics.chem-ph},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2015arXiv151007512R},
  keywords     = {Physics - Chemical Physics},
}

@InProceedings{43146,
  author    = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
  title     = {Machine Learning: The High Interest Credit Card of Technical Debt},
  booktitle = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)},
  date      = {2014},
}

@Article{2016arXiv160407316B,
  author       = {{Bojarski}, M. and {Del Testa}, D. and {Dworakowski}, D. and {Firner}, B. and {Flepp}, B. and {Goyal}, P. and {Jackel}, L.~D. and {Monfort}, M. and {Muller}, U. and {Zhang}, J. and {Zhang}, X. and {Zhao}, J. and {Zieba}, K.},
  title        = {{End to End Learning for Self-Driving Cars}},
  journaltitle = {ArXiv e-prints},
  date         = {2016-04},
  eprint       = {1604.07316},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2016arXiv160407316B},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
}

@Online{github:lrp,
  author  = {Sebastian Lapuschkin},
  url     = {https://github.com/sebastian-lapuschkin/lrp_toolbox},
  urldate = {2017-05-20},
}

@Online{github:lrp_tf,
  author  = {Vignesh Srinivasan},
  url     = {https://github.com/VigneshSrinivasan10/interprettensor},
  urldate = {2017-05-20},
}

@Online{github:tf,
url = {https://github.com/tensorflow/tensorflow},
urldate = {2017-06-05},
}

@InBook{Binder2016,
  author    = {Binder, Alexander and Bach, Sebastian and Montavon, Gregoire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  title     = {Layer-Wise Relevance Propagation for Deep Neural Network Architectures},
  booktitle = {Information Science and Applications (ICISA) 2016},
  date      = {2016},
  editor    = {Kim, Kuinam J. and Joukov, Nikolai},
  publisher = {Springer Singapore},
  location  = {Singapore},
  isbn      = {978-981-10-0557-2},
  pages     = {913--922},
  doi       = {10.1007/978-981-10-0557-2_87},
}

@Online{github:lrp_tf2,
  author  = {Dan Shiebler},
  url     = {https://github.com/dshieble/Tensorflow_Deep_Taylor_LRP},
  urldate = {2017-05-20},
}

@Online{github:tf_dae,
author = {Gabriele Angeletti},
url = {https://gist.github.com/blackecho/3a6e4d512d3aa8aa6cf9},
urldate = {2017-06-05},
}

@Article{doi:10.1063/1.3134744,
  author       = {Jeff R. Hammond and Karol Kowalski},
  title        = {Parallel computation of coupled-cluster hyperpolarizabilities},
  journaltitle = {The Journal of Chemical Physics},
  date         = {2009},
  volume       = {130},
  number       = {19},
  pages        = {194108},
  doi          = {10.1063/1.3134744},
  eprint       = {http://dx.doi.org/10.1063/1.3134744},
}

@Proceedings{ICML08,
  title     = {Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08)},
  date      = {2008},
  editor    = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.},
  publisher = {ACM},
  booktitle = {Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08)},
}

@Article{doi:10.1021/acs.jcim.7b00083,
  author       = {Nakata, Maho and Shimazaki, Tomomi},
  title        = {PubChemQC Project: A Large-Scale First-Principles Electronic Structure Database for Data-Driven Chemistry},
  journaltitle = {Journal of Chemical Information and Modeling},
  date         = {0},
  volume       = {0},
  number       = {0},
  pages        = {null},
  note         = {PMID: 28481528},
  doi          = {10.1021/acs.jcim.7b00083},
  eprint       = {http://dx.doi.org/10.1021/acs.jcim.7b00083},
}

@Article{PER-GRA:2007,
  author       = {P\'erez, Fernando and Granger, Brian E.},
  title        = {{IP}ython: a System for Interactive Scientific Computing},
  journaltitle = {Computing in Science and Engineering},
  date         = {2007-05},
  volume       = {9},
  number       = {3},
  pages        = {21--29},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2007.53},
  url          = {http://ipython.org},
  publisher    = {IEEE Computer Society},
}

@Online{jupyter,
  url     = {http://jupyter.org/},
  urldate = {2017-06-05},
}

@Online{figshare,
url = {https://figshare.com/},
urldate = {2017-06-05},
}

@Article{2003cs........8031G,
  author       = {{Gershenson}, C.},
  title        = {{Artificial Neural Networks for Beginners}},
  journaltitle = {eprint arXiv:cs/0308031},
  date         = {2003-08},
  eprint       = {cs/0308031},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl       = {http://adsabs.harvard.edu/abs/2003cs........8031G},
  keywords     = {Neural and Evolutionary Computing, Artificial Intelligence, C.1.3, I.5.1},
}

@Article{Smith_2017_3192,
  author        = {Smith, J. S. and Isayev, O. and Roitberg, A. E.},
  title         = {ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost},
  journaltitle  = {Chem. Sci.},
  date          = {2017},
  volume        = {8},
  issue         = {4},
  pages         = {3192-3203},
  doi           = {10.1039/C6SC05720A},
  __markedentry = {[flareon:]},
  abstract      = {Deep learning is revolutionizing many areas of science and technology{,} especially image{,} text{,} and speech recognition. In this paper{,} we demonstrate how a deep neural network (NN) trained on quantum mechanical (QM) DFT calculations can learn an accurate and transferable potential for organic molecules. We introduce ANAKIN-ME (Accurate NeurAl networK engINe for Molecular Energies) or ANI for short. ANI is a new method designed with the intent of developing transferable neural network potentials that utilize a highly-modified version of the Behler and Parrinello symmetry functions to build single-atom atomic environment vectors (AEV) as a molecular representation. AEVs provide the ability to train neural networks to data that spans both configurational and conformational space{,} a feat not previously accomplished on this scale. We utilized ANI to build a potential called ANI-1{,} which was trained on a subset of the GDB databases with up to 8 heavy atoms in order to predict total energies for organic molecules containing four atom types: H{,} C{,} N{,} and O. To obtain an accelerated but physically relevant sampling of molecular potential surfaces{,} we also proposed a Normal Mode Sampling (NMS) method for generating molecular conformations. Through a series of case studies{,} we show that ANI-1 is chemically accurate compared to reference DFT calculations on much larger molecular systems (up to 54 atoms) than those included in the training data set.},
  publisher     = {The Royal Society of Chemistry},
  timestamp     = {2017.06.11},
}

@Article{Schuett_2017_13890,
  author        = {Schütt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and Müller, Klaus R. and Tkatchenko, Alexandre},
  title         = {Quantum-chemical insights from deep tensor neural networks},
  date          = {2017-01},
  volume        = {8},
  pages         = {13890},
  doi           = {10.1038/ncomms13890},
  __markedentry = {[flareon:6]},
  publisher     = {The Author(s)},
  timestamp     = {2017.06.11},
}

@Comment{jabref-meta: databaseType:biblatex;}

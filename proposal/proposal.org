#+title: Deciphering the Contents of Chemically-Trained Neural Networks into Physical Intuition
#+author: Eric Berquist
#+date: June 15th, 2017
#+options: toc:nil author:t creator:nil email:nil title:t
#+latex_class: article
#+latex_class_options: [12pt]
#+latex_header: \input{./preamble.tex}

#+begin_export latex
\begin{anfxnote}{panel review}
points from panel review:

- Another point that did not become very clear was how you would construct the "novel" machine learning approach in aim \#3. Can you explain what would be novel about your approach?

-> The point is not to apply a novel ML architecture to QC for improved prediction. The point is to apply a hypothetically better ML architecture to QC to see if better or worse prediction performance correlates with changes in input feature relevance.
\end{anfxnote}
#+end_export

# A measure such as ROC is a good statistical metric for evaluating the quality of the learned model, but is only an indirect probe of the learned parameters. In order to even qualitatively understand the dependence of model quality on the input featurization, features would have to be added and removed in a combinatorial process.

# desire something with direct relationship between featurization and

# really need to read about input featurization/automatic feature generation

* Overview and Objectives

The explosive growth of computing power has enabled the use of machine learning (ML) models for the accurate calculation of chemical properties. Current ML models are capable predicting energetic properties of small, neutral organic molecules to almost 1 kcal/mol \fxnote{[REF]}. However, it is unclear /what/ the quality of a prediction will be when a ML model is given a molecule outside its training set, and /why/ the model is giving that prediction. Each ML model is a set of unknown parameters that constitute a black box \cite{wiki:blackbox}, where it is not known or understood /how/ the model functions, /why/ the model gives the predictions it does, and _whether or not correct predictions are being made for the right reasons_\cite{2017arXiv170303717S}. A critical step for the continued application of ML models to quantum chemistry will be to make predictions beyond the relatively simple example systems that are currently seeing widspread use \cite{Ramakrishnan:2014ij}. Some of the most important, interesting, and unexplored chemistry in terms of reactivity and intrinsic molecular properties involves species that are either charged or have unpaired electrons, both of which are outside the scope of current applications. If a model is not right for the right reasons, there cannot be an expectation that it is transferable to even similar inputs, and is an indicator that the architecture is insufficient for training on more complex datasets. Such a battle is being fought within electronic structure theory\cite{Zhao2008,Medvedev49,Kepp496,Medvedev496}, with the most negative potential consequence being invalidation of the past decade's application of quantum chemical methods to chemical problems.

The _overall objective_ is to quantify what quantum chemistry-trained machine learning models are actually learning. The _central hypothesis_ is that the application of machine learning to quantum chemistry is physically motivated, /i.e./, ML models are learning parameters that connect the molecular representation (input features) to predictions in a manner that is qualitatively identical to how a trained chemist would apply their intuition. An implicit connection has already been made \cite{2017arXiv170205532F} between how a molecule is represented as input to ML models and the accuracy of prediction for certain kinds of properties, but this connection has not been explored. The _rationale_ is that by understanding the connection between the molecular representation given to ML models and their predictions, ML models themselves can be used for rationalizing molecular structure-property relationships. The overall objective will be achieved through the following _specific aims_:

1. *Reproduce existing machine learning models from the literature.* I will replicate two neural networks (NNs) from \parencite{2017arXiv170205532F} to serve as both the necessary architecture that analysis tools developed later in this proposal will be built on, and as the baseline for \fxnote{the quality of the results}. NNs are chosen since they overall exhibit the most quantitatively accurate predictions compared to other ML architectures when trained on quantum chemical data. The properties that I will reproduce from the literature are the isotropic static polarizability \(\bar{\alpha}\) \parencite{POC:POC407} and the zero-point vibrational energy \(E_\text{ZPVE}\) \parencite{doi:10.1063/1.2436891}, which represent molecular properties that can be derived from spectroscopic observables and are difficult to predict with current NN approaches.

2. *Characterize the parameters learned by existing machine learning models from the literature.* I will use relevance propagation \cite{10.1371/journal.pone.0130140,Binder2016,JMLR:v17:15-618} to quantify the relative importance of each input feature for each predicted property of the NNs in aim #1. Relevance propagation gives insight into a model's learned coefficients in a fashion that can be used directly in statistical analysis and visualization.

3. *Train multi-stage neural networks on both existing and new properties.* I will train combined unsupervised and supervised NNs to predict vibrational and nonlinear optical properties of the molecules used in aim #1. These properties are \(\bar{\alpha}\), \(E_\text{ZPVE}\), the static parallel first hyperpolarizability \(\beta_{\parallel}\) \parencite{doi:10.1063/1.3134744}, and the set of frequencies \(\{\tilde{\nu}\}\) that compose stick vibrational spectra. The first two properties will validate the performance of these new NNs against existing literature predictions, and the second two properties are a natural extension in complexity of the first two properties. Unsupervised NNs are capable of discovering intrinsic properties of their inputs \cite{VincentPLarochelleH2008}, and adding them to the ML pipeline increases the likelihood that the models will learn "chemical" parameters.

4. *Characterize the parameters learned by multi-stage neural networks.* I will use the relevance propagation techniques from aim #2 to show the underlying causes for the performance of the NNs from aim #3 on both the existing and novel property predictions.

The _expected outcomes_ are

- Clear connections between the input molecular representations and predicted outputs that can be used to build further quantitative structure-function relationships.
- Adding unsupervised learning stages improves the prediction accuracy of ML models \fxnote{in a form that appears analogous to chemical intuition}.
- Well-defined and publicly-available protocols for applying more complex ML models to chemical properties, regarding all stages of the prediction pipeline: generation of the QM data from calculations, preparation of the input, training of the model, and analysis of the results.
- ML models capable of predicting complex molecular properties, with the first proof-of-concept predictions of higher-order nonlinear optical properties and vibrational spectra.

* Significance

This is the first attempt at understanding the parameters of ML models used to predict microscopic and macroscopic molecular properties, rather than treating the models as black boxes that cannot be understood. If ML models are shown to be learning physically-motivated or chemically-intuitive parameters, ML can become more than just a path for the accurate prediction of molecular properties, but be transformed into a tool itself that can give deep insight into molecular structure-property relationships. Training ML models is expensive in terms of both human and CPU time\cite{TODO}, with substantial technical hurdles mounting over time\cite{43146}. If this proposal shows that ML models are not learning properly, then the scientific research community can avoid the wasteful use of resources on model training and shift their focus to developing better model architectures than the current state-of-the-art.

* Background

** Machine Learning

Machine learning is the ability for computers to "learn" without being given explicit instructions. Rather than providing exact instructions though traditional programming, computers are fed sets of input data and are usually expected to return a certain result. By training itself to reproduce results, a learned ML model would ideally be able to predict outputs for new, unknown inputs. Common applications of ML are in email spam filtering, search engine prediction, image and voice recognition, and self-driving cars.

Some definitions and terms that are used throughout this proposal are

- /Architecture/: the formal structure of the network or ML model itself, encompassing the region from equations and diagrams to the implementation (code).
- /Model/: here, an implemented architecture (in code) with learned parameters.
- /Pipeline/: multiple steps and components chained together, such as the preparation of data for input into an architecture, the architecture itself, and any steps required to transform the architecture output into something else useful, such as visualizations or statistics.

There are two categories of learning discussed throughout this proposal:

- /Supervised learning/: Train a machine learning model using data where the correct output prediction is known and given for each input sample, and the goal of the model is to predict similar types of outputs for new inputs \cite{\fxnote{REF}}.
- /Unsupervised learning/: Train a machine learning model using data where the correct output prediction is not given, and the goal of the model is to learn intrinsic properties of the inputs by recreating the input as output \cite{\fxnote{REF}}.

# There is a third category of learning called reinforcement learning; since there have not yet been any applications of reinforcement learning to quantum chemical problems, it will not be touched upon in the remainder of the proposal.

** Neural Networks

\begin{anfxnote}{neural networks background}
Short background on (artificial) neural networks; goal, general structure (w/ figure) and terminology, how do they learn (backpropagation)
- It did not become clear why identifying the inner workings of machine learning approaches, i.e., identifying whether an ML approach gets the right answer for the right reasons, has any practical relevance.
- Could you explain more what is the payoff from identifying these inner workings?
- Would this help with constructing more transferable machine learning approaches?
I think here is where these points get touched on. Will have to mention non-transferability and overfitting.

- Note that we are interested in feed-forward networks, not recurrent networks. There are many forms of ANNs, both in global structure, layer type, and neuron type, but we focus on this particular structure to maintain compatibility with previous work.

- Autoencoders perform
- Denoising autoencoders differ from autoencoders by
- Connection between feature generation and dimensionality reduction?
\end{anfxnote}

# can do :float wrap in attr_latex
#+caption: Cartoon represtation of a feedforward neural network, with the input layer on the bottom, hidden layer in the middle, and output layer on the top. Taken from \parencite{blog:dnn2}. \fxnote{Something like this but cleaner.}
[[file:single_hidden_layer_nn.png]]

The neural network architectures implied by the two types of learning above lead to the capability of chaining them together in a pipeline, where an unsupervised NN is trained for some amount of time in a "pretraining" step, and the bottom layers (closest to the input) in an unsupervised NN are replaced with the contents of the unsupervised NN, rather than being initialized with random weights.

** Machine Learning in Chemistry

The use of machine learning to make chemical predictions is not new, with work dating back over 25 years for prediction of NMR spectra using small neural networks trained on experimental data \cite{THOMSEN1989212}. The largest application of machine learning to chemical problems is within cheminformatics, where it has seen wide use within industrial drug discovery with emphasis on predicting quantitative structure-activity relationships (QSAR) \cite{doi:10.1080/17460441.2016.1201262}. The goal is to predict the activity of a given drug candidate based on experimental activities of many other molecules, with inputs being information about atom types, bond types, number of aromatic rings, atomic partial charges, and other pieces of structural information, all of which are related to the molecular graph or connectivity \cite{Kearnes2016}.

In particular, there is a recent application of deep neural networks (DNNs) to QSAR datasets\cite{doi:10.1021/ci500747n}, which contains a systematic study for determining the best model parameters. The machine learning community calls this "hyperparameter tuning", which is another term for parameter optimization. However, this is still an empirical black-box approach, where the input is \fxnote{carefully controlled} and statistical analysis is performed on the output, but this does not provide enough insight into how or why the quality of a model changes. For example, whether or not a rectified linear unit (ReLU) or sigmoid unit is the best function to represent neuron activation says nothing about why one molecule may be more potent than another in a QSAR study. This brute-force type of parameter optimization /does/ provide a good starting point for understanding the sensitivity of a ML model. Unfortunately, even parameter optimization has not been extensively performed on models trained using quantum chemical data. In that sense, cheminformatics is a step ahead of other sub-disciplines in chemistry regarding the /application/ of machine learning models, but not in the /understanding/ of their models. 

Their parameter optimization study is especially relevant to this proposal because it examines the effect of placing an unsupervised NN before other NNs for unsupervised pretraining. Surprisingly, the authors found that an unsupervised pretraining step decreased the accuracy of their predictions, which is counter to the expected outcome of this proposal. However, the paper implies that their results are not even valid due to algorithmic restrictions in their software. Therefore it seems incorrect to draw any conclusions from this, such as "no unsupervised pretraining is needed". It would be interesting to see if the same conclusion is drawn for models trained on quantum chemical data using the proper algorithms, as will be done in this proposal.

Additionally, it is unclear why a DNN trained on the combination of all QSAR datasets (called a "joint DNN") performs better than separate DNNs for each dataset when considering the lack of overlap in the training sets. The methods developed in this proposal, while being applied to models trained on quantum chemical data, should be applicable to any DNN (consider that relevance propagation is mostly developed in computer vision/image recognition). One goal of this proposal is to transfer the idea of relevance propagation from its original intended application field to other fields. If it is indeed transferable, then it may shed some light on why unsupervised learning resulted in decreased prediction performance and the improvement of join DNNs over separate DNNs.

# Therefore, this proposal is making the assumption that models trained on quantum chemical data found in the literature are using satisfactory parameters, which we have no way of knowing without doing our own hyperparameter tuning. However, parameter optimization is not the focus of this particular proposal and would be a separate work. One could argue that if the literature models are poor predictors and/or are xxx, then relevance propagation is meaningless, however the final output from literature models so far has reasonable (DFT-level) errors on most predicted properties.

# "A surprising observation from Figure 5 is that the neural network achieved the same average predictive capability as RF when the network has only one hidden layer with 12 neurons. This size of neural network is indeed comparable with that of the classical neural network used in QSAR. This clearly reveals some key reasons behind the performance improvement gained by our way of using DNN: (1) a large input layer that accepts the thousands of descriptors without the need for feature reduction and (2) the dropout method that successfully avoids over fitting during training."

# I don't think that this is an adequate explanation for the success of the smallest model.

** Machine Learning in Quantum Chemistry

\begin{anfxnote}{background for ML in QC}
However, the use of machine learning in quantum chemistry, specifically electronic structure theory, is relatively new, with the earliest references on Scopus dating back to 2008, a large spike in 2013, and rapid growth from 2015 to today. The goal is to predict more elemental properties than within cheminformatics, such as the internal energy, enthalpy, free energy, heat capacity, HOMO and LUMO energies and gaps, dipole moments, polarizabilities, and zero-point vibrational energies\cite{2015arXiv150204563R}. More advanced applications are the use of neural networks for predicting the products of organic reactions\cite{doi:10.1021/acscentsci.6b00219} and the transport properties of candidates for organic photovoltaics\cite{C5SC04786B}.

- Talk about message passing?\cite{2017arXiv170401212G}

There have not been attempts to predict tensorial properties, just scalar-valued properties. This precludes the prediction of full spectroscopic properties, which are mathematically not representable as single scalars. There is recent work considering the prediction of full spectra, specifically linear vibrational spectra from /ab initio/ molecular dynamics (AIMD) simulations\cite{2017arXiv170505907G}. However, this proposal is concerned with the generation of spectra from static calculations, which avoids some convolution of the calculated spectra being dependent on the model's learned representation of the potential energy surface. Additionally, their vibrational spectra were calculated from the dipole autocorrelation function, which is dependent on artificially partitioning the electron density into atomic charges, which they derive from the neural network. Thus, this is not an end-to-end \cite{2016arXiv160407316B} prediction of molecular spectra from a single structure, as will be performed in this proposal. By performing end-to-end prediction rather than decomposing the problem so that the neural network only considers part of the prediction task, this proposal pushes the limits of attempting to train neural networks on the prediction of complex molecular properties.
\end{anfxnote}

** Relevance Propagation

Layer-wise relevance propagation (LRP, or relevance propagation) is a method for identifying what a ML model has learned \cite{10.1371/journal.pone.0130140} in terms of the model's input features. Figure [[relevance-propagation-lit-example]] is a concrete example of what the output from relevance propagation looks like when applied to image classification by a neural network. Here, we assume that the network correctly identified the subject of the image as a cat (rather than a dog or a potted plant), but relevance propagation shows which image pixels were most important for the network to determine the photo is of a cat. The pixel-wise importance is a single number for each pixel that can be interpreted as a contribution for that pixel to the final classification of the image. More generally, is it the relative importance of each input feature to the predicted output; here and in other image recognition examples, pixels are input features. Applications to image classification resulting in pixel importance naturally lends itself to visualizing the output as a heatmap on top of the original input.

#+name: relevance-propagation-lit-example
#+caption: Example of output from relevance propagation showing which sections of an image the neural network considered important during classification. Taken from \parencite{10.1371/journal.pone.0130140}.
[[file:2-Figure1-1.png]]

\begin{anfxnote}{other methods}
Other methods exist for assigning rules of how input features map to predictions \cite{Finnegan105957,2017arXiv170303717S,2016arXiv161107478L}. Several of these are based on the idea of gradient perturbations, where repeated changes in prediction are measured as a result of small changes in the input. Performed enough times, this creates a map of the network's decision boundary\cite{wiki:db}. A gradient perturbation-based method is unsatisfactory because it requires repeated forward passes through the network with a set value for the perturbation size, and relevance propagation requires only one backward pass with no free parameters. Additionally, most methods for assigning input relevance have only been used for image classification, where the input features are of uniform type (pixel data). The input featurization for representing molecular structures\cite{2017arXiv170205532F} is heterogeneous, and it is unclear how the perturbation parameter should be varied for each kind of molecular feature.
\end{anfxnote}

Although no improvements will be made to the basic relevance propagation algorithm itself, there is novelty in two areas. To the best of the author's knowledge, this is the first time relevance propagation will be applied to a regression task rather than a classification task, and the first time relevance propagation will be applied outside of image classification or computer vision. A potential connection between the heatmap representation and hallucinations from generative adversarial networks (GANs), which have been applied to quantum chemistry \cite{doi:10.1063/1.4973380} is an interesting future research area.

\begin{anfxnote}{Kearnes paper}
The closest use of input relevance in molecular predictions is monitoring the evolution of input features as the training of a network\cite{Kearnes2016}, shown in [[fig:input-feature-evolution]]. However, there is no definition for what the "evolution of input features" is, such as the metric for evolution, or what the units are. This proposal is also not concerned with how the importance changes as a function of network training, only the final explanation of the fully-trained network predictions.
\end{anfxnote}

#+name: fig:input-feature-evolution
#+caption: Example of how a molecule (ibuprofen) is mapped to input features, and how the input features change as time passes during network training. Taken from \parencite{Kearnes2016}.
[[file:10822_2016_9938_Fig8_HTML.png]]

* Research Plan

** Specific Aim #1: Reproduction of Existing Literature Neural Networks

*** Introduction

- The _objective_ is to reproduce trained neural networks from the literature in order to create the foundation of the machine learning pipeline to be developed within this proposal and to serve as a validation baseline for further predictions.

- The _hypothesis_ is that published quantum chemical neural networks are entirely reproducible by connecting free, open-source tools.

- To test this hypothesis, I will reproduce the ML pipeline and results from \parencite{2017arXiv170205532F}, specifically the isotropic static polarizability \(\bar{\alpha}\) and the zero-point vibrational energy \(E_{\text{ZPVE}}\).

- The _expected outcome_ is a fully-worked and documented reproduction of neural networks from the literature that can serve as the basis for not only this proposal's later aims, but for future pipelines within the wider chemistry and machine learning communities.

*** Research Design

Unfortunately, the learned models for the results presented in \parencite{2017arXiv170205532F} are not available, only descriptions of the architectures. Recreating the literature models requires an implementation of the model architecture and input data in the proper format.

There are two neural network-based architectures described in \parencite{2017arXiv170205532F}: Graph Convolutions \cite{Kearnes2016} (GC) and Gated Graph Neural Networks \cite{2015arXiv151105493L} (GG). These NN architectures are used again as baselines in \parencite{2017arXiv170401212G}. Since the original GC implementation referenced in \parencite{2017arXiv170205532F} is [[https://github.com/tkipf/gcn][openly available]]\cite{kipf2016semi}, I will use the GC-based architecture with modifications described in section E5 of \parencite{2017arXiv170205532F}. \fxnote{Is it safe to just reference this paper, or are more details necessary?} Details for the GC architecture input, called the Molecular Graph representation, are shown in tables 1 and 2 of \parencite{2017arXiv170205532F} and reproduced here.

# #+begin_quote
# \begin{anfxnote}{section E5}
# We use the Graph Convolutions model as described in~\cite{Kearnes2016} with several structural modifications and optimized hyperparameters for this problem.

# The graph convolution model is built on the concepts of ``atom'' layers (one real vector associated with each atom) and ``pair'' layers (one real vector associated with each pair of atoms). The graph convolution architecture defines operations to transform atom and pair layers to new atom and pair layers.

# There are three structural changes to the model compared to the one described in~\cite{Kearnes2016}. We describe these briefly here with details in the Supplementary Material. First, we removed the ``Pair order invariance'' property by simplifying the ($A \rightarrow P$) transformation.  Since the model only uses the atom layer for the molecule level features, pair order invariance is not needed.

# The second structural change was in the use of the euclidean distance between atoms. In the ($P \rightarrow A$) transformation, we divide the value from the convolution step by a series of distance exponentials. That is, if the original convolution for an atom pair $(a, b)$ with distance $d$ produced a vector $V$, we concatenate the vectors $V$, $\frac{V}{d^{1}}$, $\frac{V}{d^{2}}$, $\frac{V}{d^{3}}$, and $\frac{V}{d^{6}}$ to produce the transformed value for the pair $(a, b)$.

# The third structural change is from other work on using neural networks on chemical graphs~\cite{duvenaud2015convolutional}. Inspired by fingerprinting like Extended Connectivity Fingerprints~\cite{rogers2010extended}, the authors use a sum of softmax operations to convert a real valued vector to a sparse vector and sum those sparse vectors across all the atoms. We use the same operation here along with a simple sum across the atoms to produce molecule level features from the top atom layer. We found this worked as well or better than the Gaussian histograms first used in Graph Convolutions~\cite{Kearnes2016}.

# To optimize the network, we did a hyperparameter search using Gaussian Process Bandit Optimization~\cite{JMLR:v15:desautels14a} as implemented by HyperTune~\cite{hypertune}. The parameters, the search ranges, and the values chosen for the results in this paper are listed in the Supplementary Material. Note that the hyperparameter search was based on the evaluation of the validation set for a single fold of the data.

# We optimized using the ADAM optimizer~\cite{kingma2014adam} with 10 simultaneous replicas, a learning rate of 0.01 (decayed by 0.96 every 2 epochs), and a batch size of 96 for 250k steps. We tested the models performance at various points during training and selected the step with the lowest error on the validation set.
# \end{anfxnote}
# #+end_quote

# *** From April:

# #+BEGIN_QUOTE
# Each model and target combination was trained using a uniform random hyper parameter search with 50 trials. \(T\) was constrained to be in the range \(3 \leq T \leq 8\) (in practice, any \(T \geq 3\) works). The number of set2set computations \(M\) was chosen from the range \(1 \leq M \leq 12\). All models were trained using SGD with the ADAM optimizer (Kingma & Ba (2014)), with batch size 20 for 2 million steps (360 epochs). The initial learning rate was chosen uniformly between \(1e^{-5}\) and \(5e^{-44}\). We used a linear learning rate decay that began between 10% and 90% of the way through training and the initial learning rate \(l\) decayed to a final learning rate \(l*F\), using a decay factor \(F\) in the range \([0.1, 1.0]\).

# The QM-9 dataset has 130462 molecules in it. We randomly chose 10,000 samples for validation, 10,000 samples for testing and used the rest for training. We use the validation set to do early stopping and model selection and report scores on the test set. All targets were normalized to have mean 0 and variance 1. We minimized the mean squared error between the model output and the target, although we evaluate mean absolute error.
# #+END_QUOTE

#+label: table:mg-input-rep-atoms
#+caption: The Molecular Graph (MG) input representation: single atom features
#+attr_latex: :environment tabu :align |lp{10.25cm}l|
| Feature          | Description                                                                | Size |
|------------------+----------------------------------------------------------------------------+------|
| Atom type        | H, C, N, O, or F (one-hot)                                                 |    5 |
| Chirality        | R or S (one-hot or null)                                                   |    2 |
| Formal charge    | Integer electronic charge                                                  |    1 |
| Partial charge   | Calculated partial charge                                                  |    1 |
| Ring sizes       | For each ring size (3-8), the number of rings that include this atom       |    6 |
| Hybridization    | sp, sp\(^2\), or sp\(^3\) (one-hot or null)                                |    3 |
| Hydrogen bonding | Whether this atom is a hydrogen bond donor and/or acceptor (binary values) |    2 |
| Aromaticity      | Whether this atom is part of an aromatic system                            |    1 |
|------------------+----------------------------------------------------------------------------+------|
|                  |                                                                            |   21 |

#+label: table:mg-input-rep-pairs
#+caption: The Molecular Graph (MG) input representation: atom pair features
#+attr_latex: :environment tabu :align |lp{10.25cm}l|
| Feature          | Description                                                                                                                                    | Size |
|------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------|
| Bond type        | Single, double, triple, or aromatic (one-hot or null)                                                                                          |    4 |
| Graph distance   | For each distance (1-7), whether the shortest path between the atoms in the pair is less than or equal to that number of bonds (binary values) |    7 |
| Same ring        | Whether the atoms in the pair are in the same ring                                                                                             |    1 |
| Spatial distance | The Euclidean distance between the two atoms                                                                                                   |    1 |
|------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------|
|                  |                                                                                                                                                |   13 |

# #+caption: The Molecular Graph (MG) input representation: single atom features, reproduced from Table 1 of \parencite{2017arXiv170401212G}.
# | Feature             | Description                    |
# |---------------------+--------------------------------|
# | Atom type           | H, C, N, O, F (one-hot)        |
# | Atomic number       | Number of protons (integer)    |
# | Partial charge      | Calculated charge (float)      |
# | Acceptor            | Accepts electrons (binary)     |
# | Donor               | Donates electrons (binary)     |
# | Aromatic            | In an aromatic system (binary) |
# | Hybridization       | sp, sp2, sp3 (one-hot or null) |
# | Number of hydrogens | (integer)                      |

The QM9 dataset consists of 134K molecules \cite{Ramakrishnan:2014ij} containing up to 9 heavy atoms from the elements H, C, O, N, and F, with a maximum number of 29 atoms. The representation in tables [[table:mg-input-rep-atoms]] and [[table:mg-input-rep-pairs]] will result in an input size of \(21\binom{x}{1} + 13\binom{x}{2}\) for a given number of atoms \(x\), leading to a total length of 5,887 for the maximum number of 29 atoms in QM9. Inputs are available as modified XYZ files from the [[http://quantum-machine.org/datasets/][Quantum Machine (http://quantum-machine.org/)]] website under the [[http://figshare.com/collections/Quantum_chemistry_structures_and_properties_of_134_kilo_molecules/978904][QM9 Dataset]] section\cite{Ramakrishnan:2014ij,doi:10.1021/ci300415d}, which will be transformed into the Molecular Graph (MG) representation using RDKit\cite{rdkit} with Gasteiger partial charges as in \parencite{Kearnes2016}.

- Modify the original Graph Convolutions architecture to the one described in Section E5 of \parencite{2017arXiv170205532F}.
- Using the model parameters described in that section, train two separate models, one for the isotropic static polarizability \(\bar{\alpha}\), and another for the zero-point vibrational energy \(E_\text{ZPVE}\).

For the reproduction of literature results, the only numerical values from \parencite{2017arXiv170205532F} are in Table 3, which shows the mean absolute error (MAE) for each input representation-architecture combination. Because the sample size of QM9 is sufficiently large (134K molecules), the MAE is calculated using out-of-sample validation, where the ML models are trained using 90% of the available data and compared against the DFT (B3LYP/6-31(2df,p)) results for the remaining 10%. The 90% constitutes ~117K molecules after removing 3K from 134K due to failed SMILES consistency tests. This 90/10 (training + validation)/test set split allows for 10-fold cross-validation. It is not mentioned how the concrete splits are obtained or how the final MAE is calculated from the 10 models. For this proposal, I will perform an unbiased random shuffle of QM9 index codes and split them into 10 uniform bins. After training and model validation using the procedure described above, the final MAE will be calculated as the mean of the 10 individual MAEs. \fxnote{Is this sufficient? If so, are there more technical terms for these procedures?} The literature models will be considered replicated if the two final models I train have MAEs within 95% of 0.227 \(a_{0}^{3}\) for the polarizability and 0.00975 eV for the ZPVE, respectively. \fxnote{Is there a better error metric than 95\% of a single number? Seems very unsatisfactory}

\begin{anfxnote}{final training}
There is some ambiguity to me here. If I wanted to perform a prediction of one of these molecular properties, would I then train an 11th model using all 100\% of the available sample data? What is the ``final'' model?
\end{anfxnote}

# Starting from the ~131k molecules in QM9 after removing the ~3k molecules (see above) we have created a number of train-validation-test splits. We first split the dataset into test and non-test (training + validation) sets and vary the percentage of data in test to explore the effect of amount of data in error rates (see Results). Then inside the non-test set, we do 10 fold cross validation for hyperparameter optimization. That is for each model 90% (the training set) of the non-test set is used for training and 10% (the validation set) is used for hyperparameter selection. For each test/non-test split, we then have 10 models trained on different subsets for the non-test set and we report the MAE on the test set across those 10 models.

*** Expected Outcomes

The concrete products of this aim will be a set of Python scripts that transform the XYZ-like files into the Molecular Graph representation, implement the modified Graph Convolutions architecture, train MG/GC models for each molecular property (\(\bar{\alpha}\) and \(E_\text{ZPVE}\)) using out-of-sample cross-validation, and calculate each molecular property from the trained models when given a normal XYZ molecular structure. This will be a fully-worked and documented reproduction of neural networks from the literature that can serve as the basis for not only this proposal's later aims, but for future pipelines within the wider chemistry and machine learning communities. These scripts will take the form of Jupyter Notebooks \cite{jupyter,PER-GRA:2007}, which combine code, math, and documentation in an easy-to-replicate package that is popular in the machine learning community.

** Specific Aim #2: Characterization of Existing Literature Neural Networks

# *** From lecture

# - Specific Aim 1: Title (Formulate as Goal)
#     - Introduction
#     - [Preliminary Results]
#     - Research Design
#     - Expected Outcomes
#     - Potential Problems / Alternate Approaches

# - Repeat for other aims (2-3 pages each)
# - End with research timeline and brief conclusion (optional)

# - Introduction (1 paragraph)
#     - State objective of work in this aim
#     - Relate objective to problem / central hypothesis / gap
#     - State working hypothesis of aim
#     - Summarize what will be done to test hypothesis
#     - Summarize outcomes and their impact

# - Research Design (general considerations)
#     - Plan should be specific
#     - Appropriate level of detail
#     - Simple, declarative sentences
#     - Strong verbs ("expect", "will") over weak ("try")
#     - Treat each paragraph as unit
#         - Set of activities focused on single goal
#         - Make sure both activities and their point are clear

# - Research Design (tips):
#     - State hypotheses underlying individual experiments, where appropriate
#     - Consider interdependence of experiments
#     - Project design encompasses all likely outcomes, not just desired
#     - Scope of activities will collectively attain aim's objective

# - Expected Outcomes (1-2 paragraphs)
#     - Highlight payoff from work in the aim
#     - Expand on outcome sentence in Introduction paragraph
#     - Consider results from individual experiments
#     - Relate outcomes back to pg. 1-2
#         - Assume best-case scenario for success
#         - Be enthusiastic, but realistic

# - Potential Problems
#     - More later...
#     - For now, think about weakest points in plan

*** Introduction

- The _objective_ is to quantify what already-published neural network-based ML models have learned.

- The _hypothesis_ is that when predicting an output, the most important (relevant) parts of the input for that output align with our trained chemical intuition. Specifically, for strongly geometry-dependent properties, such as the ZPVE, more relevance will be placed on geometric input features such as bond lengths and bond types. For strongly wavefunction- or density-dependent properties, such as the isotropic polarizability, more relevance will be placed on electronic input features such as partial atomic charges compared to other features.

- To test this hypothesis, I will develop the necessary ML pipeline for adding relevance propagation and analysis steps to the already-published ML models. This will involve connecting existing relevance propagation tools \cite{JMLR:v17:15-618,github:lrp,github:lrp_tf,github:lrp_tf2} to the end of the pipeline from aim #1 and creating a human-understandable representation of the relevance propagation output in terms of molecular features.

*** Research Design

The authors of the relevance propagation algorithm I will use have created an open-source reference implementation in Python. From their GitHub page\cite{github:lrp}:

#+BEGIN_QUOTE
The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. The LRP Toolbox provides simple and accessible stand-along implementations of LRP for artificial neural networks supporting Matlab and Python.
#+END_QUOTE

This reference implementation is interfaced with its own implementation of composable neural networks. This is not immediately compatible with the TensorFlow-based implementation of the Graph Convolutional neural networks trained in aim #1. However, there is also open-source initial interface code available for connecting TensorFlow-based models with LRP\cite{github:lrp_tf,github:lrp_tf2}. I will use these three implementations to write a Python interface between the trained GC models and LRP.

Once the LRP implementation is connected to the trained models, details of running the LRP algorithm must be considered. There are multiple decomposition variants of LRP, each with different trade-offs regarding numerical stability and conserving relevance\cite{Binder2016}. Because the number tunable parameters is small, with only one for each of the three variants, and there are examples of modulating these parameters in the literature, I will start with the variant that requires no free parameters and extend to the other variants if results do not make sense.

# \begin{anfxnote}{interpretation}
All examples from the relevance propagation literature focus on explaining image classification decisions using heat maps overlaid on the input image decision boundary. Instead of performing image classification, the neural networks trained in aim #1 perform regression using a graph-type, heterogeneous input, so a different form of interpreting the results will be needed. LRP produces output on a per-sample basis with a signed relevance value for each input feature, where the sum of all relevance values equals the prediction output. For each molecule in the QM9 dataset, I will run the LRP implementation and perform the equivalent of feature normalization and scaling so that the relevances have zero mean with a minimum and maximum corresponding to the largest absolute values in the QM9 dataset. Not all molecules in QM9 have the same number of atoms, so for each molecule I will average together the relevances over all atoms to produce a single relevance for each type of feature (12 total). While this can be viewed as a significant loss of information, the goal of this proposal is to start xxx.
# \end{anfxnote}

\fxwarning{Fact check the statement above from the sum onward. How the LRP output is normalized/manipulated may depend on if feature normalization is performed before/after training the NNs?}

#  Since the density distributions for \(\bar{\alpha}\) and \(E_{\text{ZPVE}}\) \cite{2015arXiv150204563R}
# - derive form for analyzing contributions of input features to results, such as coefficients \(\{c\}\) where \(\sum_{i}^{\text{input features}} c_{i}^{2} = 1\)
# - analyze results from relevance propagation: graphs, histograms, etc.
#    - how are the results represented straight out of the relprop algorithms? may need to do some transformations

*** Expected Outcomes

- \fxnote{Evidence for or against published ML models having learned chemically-intuitive parameters}
- \fxnote{A model ML pipeline for applying relevance propagation to quantum chemistry models}

** Specific Aim #3: Construction and Training of Novel Neural Networks

*** Introduction

- The _objective_ is to see how currently used neural network architectures perform for more complex molecular properties and how this performance changes with generative pretraining.

- To test this hypothesis, 

- The already-trained properties will be the , and the not-before trained properties will be the static parallel first hyperpolarizability, \(\beta_{\parallel}\), and full vibrational (stick) spectra, the set of frequencies \(\{\tilde{\nu}\}\).

- The _hypothesis_ is that because the multi-stage NN will be at least as sophisticated as the single-stage NN used previously in the literature, both \(E_{\text{ZPVE}}\) and \(\bar{\alpha}\) should be predicted using the multi-stage NN with equal or less error than the single-stage NN. The more complex properties \(\beta_{\parallel}\) and \(\{\tilde{\nu}\}\) are expected to have larger relative errors, in particular the set of vibrational frequencies, as predictions of the highest fundamental frequency \(\omega_1\) alone already have large errors \cite{2017arXiv170205532F}.

- Applying new ML architectures to already well-studied properties is a safety check for the architeture's use; if it performs worse than current models for existing property predictions than it cannot be expected that it will perform well for new/more complex property predictions.

*** Research Design

- Results for \(\bar{\alpha}\), \(E_{\text{ZPVE}}\), and \(\{\tilde{\nu}\}\) are already present in the labeled data that was used is inputs in aim #1 (that is, the QM9 dataset \cite{Ramakrishnan:2014ij}).

- I will use the \textsc{Dalton} quantum chemistry program package \cite{daltonpaper} for the hyperpolarizability calculations, as it is free for academic use and designed especially for the calculation of molecular response properties such as hyperpolarizabilities. These calculations will employ the B3LYP density functional in combination with the 6-31G(2df,p) basis set to maintain comparability with past calculations from the QM9 dataset \cite{Ramakrishnan:2014ij}.

- Start with the resulting (supervised) NN architectures/models from the literature that were used in aim #1.

- Build a "small" unsupervised NN architecture that can be connected to the front of the existing GC NN architecture (the "combined" architecture).

\begin{anfxnote}{nn construction}
The basic structure of the unsupervised network will be as follows. It will be a denoising autoencoder (DAE) with an input layer of the same dimension as the GC input, a coding layer of dimension ???, and a hidden layer between the input and coding layers. This will be a symmetric structure where the weights of the encoding and decoding sections are tied, meaning they are constrained to be identical. I will adapt the Molecular Graph input from aim 1 on top of this denoising autoencoder as implemented in TensorFlow\cite{tensorflow2015-whitepaper,github:tf,github:tf_dae}. Because a DAE reconstructs its inputs and does not make a classification or regression prediction, only one model needs to be trained, not four. Here I should describe how the DAE will be trained? How to decide on dimension of intermediate and coding layers? Once the DAE model is trained, it will replace the input layer for each of the trained supervised networks. If the DAE is capable of performing good input reconstruction, then the prediction performance of these combined networks should not be much worse than the supervised networks for the training, but may be improved on validation and testing.

Performance may decrease because the reconstruction of the original input cannot be perfect, which would constitute overfitting.

Some of this can go in aim 4!

Could LRP be performed on the DAE itself? Could be another test of quality for the input reconstruction

The purpose of adding the autoencoder structure is that using layer-wise relevance propagation will show the relevance of the \emph{encoded} input. The expectation is that because the learned encoding retains only important features for the input, xxx rather than using the possibly overcomplete input feature description. If the DAE is good at reconstructing the inputs

The original purposes of generative pretraining are to prevent overfitting in large neural networks and learn structure in large amounts of unlabeled data.

Larger databases already exist \cite{doi:10.1021/acs.jcim.7b00083}

In the future, with larger amount of unlabeled data (we don't want to have to run millions of hyperpolarizability calculations in order to train), such as in \parencite{doi:10.1021/acs.jcim.7b00083},

In the future, it may make sense to unfreeze the autoencoder weights and change them slightly via backpropagation, so the autoencoder learns a redudced-dimensionality encoding that is more optimal for the network output and not just reconstructing the input.
\end{anfxnote}

*** Expected Outcomes

- Models with unsupervised learning steps have improved prediction accuracy of chemical properties compared to those without. \fxnote{That is, the models developed and trained in this aim should show better prediction performance than the literature models from aim \#1. This implies the models from this aim are of higher-quality and are more likely to have "learned correctly" in the sense that they learned "chemical intutition".}

** Specific Aim #4: Characterization of Novel Neural Networks

*** Introduction

- The _objective_ is to determine the relative importance of each component in the molecular representation to predictions of complex molecular properties. This will be done by applying the analysis techniques developed in aim #1 to the neural networks trained in aim #2.

- The _hypothesis_ is that the most important input features for \(\beta_{\parallel}\) and \(\{\tilde{\nu}\}\) are similar to those for \(\bar{\alpha}\) and \(E_{\text{ZPVE}}\), respectively.

*** Research Design

\fxnote{This specific aim, as currently planned, is just the application of the analysis from aim \#1 to the models developed and trained in aim \#2. Although the goal of each specific aim sounds logical, the actual division of work between each of the specific aims seems very uneven.}

*** Expected Outcomes

- The parameters learned by ML models, and therefore their predictions, will show a strong dependence on the input features in chemically-intuitive manner.
- Neural network-based ML architectures are a valid path forward for predicting novel and more complex chemical properties.

* Broader Impacts

A crucial reason for the growth in cross-disciplinary applications of machine learning is the openness and extensiveness of introductory tools, specifically tutorials and examples. Historically, chemistry lags behind other sciences in terms of openness of procedures and results. The current infrastructure surrounding the combination of machine learning and quantum chemistry is very poor: disorganized work, disorganized results, and not all components are available for reuse. The development of these machine learning pipelines will constitute the development of open-source, freely available infrastructure that will be easily extendable. _I will provide openly *all* components of the machine learning pipeline developed in this proposed work_, including the fully-trained models, meaning the implementations using open-source software and the learned parameters for each model. All components will be placed on [[https://github.com/][GitHub]], the premier location for the open hosting of machine learning tools. Making these tools available will enable the verification of future, more advanced machine learning models that has not been possible to date. The tools will also serve as a pedagogical example for how machine learning can be applied to quantum chemical problems.

As the application of machine learning within quantum chemistry is relatively new and fast-moving, still being in the "discovery" phase, there have not been attempts at replicating machine learning pipelines, peer-reviewed or otherwise. Additionally, in traditional quantum chemistry there are a plethora of well-known program packages for performing electronic structure calculations \cite{g16,QCHEM4,daltonpaper,WCMS:WCMS93} that are self-contained, /e.g./ a single program can calculate optimized geometries, vibrational spectra, NMR chemical shifts, reaction energies, etc. This infrastructure exists to some degree for machine learning, with base packages such as scikit-learn \cite{scikit-learn} and TensorFlow \cite{tensorflow2015-whitepaper} themselves being self-contained with excellent tutorials and examples, however this infrastructure does not exist for quantum chemistry-derived machine learning models. Introductions to machine learning are numerous and extensive using the standard "fruit fly" of NNs, the MNIST database of handwritten digits \cite{lecun-01a}, and similar fully-worked introductions should exist for quantum chemistry as well. Releasing the pipeline from this proposal allows it to serve as the "fruit fly" for quantum chemistry in machine learning.

# In reality, just adding a bunch of stuff on GitHub makes the problem worse, not better, at least in my opinion. A better option would be something like a GitHub group or external organization that serves as a steward of sorts for "machine learning in chemistry", but this will never happen due to academic competion and pushback. I cannot propose the creation of such a stewardship due to this problem and the scope of the proposal; doing this above stuff on GitHub is more feasible.

\printbibliography
